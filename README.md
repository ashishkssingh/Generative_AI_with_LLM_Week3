# Generative_AI_with_LLM_Week3
Generative AI with Large Language Models course offered by Coursera in partnership with DeepLearning.AI

## Objective

This week we dive into How LLMs can be made smarter and more aligned with human goals by using human feedback to reward good behavior (RLHF), explaining their reasoning (chain-of-thought prompting), and keeping them updated with fresh information (information retrieval & augmentation). This helps them solve problems more effectively and avoid outdated knowledge.

## Topics Learned

- Reinforcement learning from human feedback
    - Aligning models with human values
    - Reinforcement learning from human feedback(RLHF)
        - Obtaining feedback from humans
        - Reward model
        - Fine-tuning with reinforcement learning
    - Proximal policy optimization (PPO)
    - How models can learn reward hacking 
    - KL Divergence to prevent reward hacking
    - Scaling human feedback
- LLM powered applications
	- Model optimization for deployment
	- GenAI lifecycle
	- How to use LLM in applications
    - Interacting with external applications
    - Helping LLMs reason and plan with chain-of-thought
    - Program-aided language models (PAL)
    - ReAct: Combining reasoning and action
    - LLM Application architectures

